---
title: "Exploratory data analysis of the default of credit card clients data set"
author: "Lara Habashy"
date: "20/11/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
library(knitr)
library(caret)
library(readxl)
set.seed(2020)
library(pacman)
library(arrow)
library(here)
p_load(
  tidyverse, skimr, feather, magrittr, lubridate,
  microbenchmark, tictoc, furrr,
  tidytext,
  xts, zoo, imputeTS,
  scales, dygraphs, plotly, htmlwidgets, viridis, ggrepel, gridExtra, ggthemes,
  tsibble
)
```

# Load the data & Preliminary Analysis
```{r load data}
raw_data <- arrow::read_feather(here("data", "raw","default_of_credit_card_clients.feather"))
dat <- janitor::row_to_names(raw_data, 1)

head(dat)
str(dat)
summary(dat)
```

The data set consists of 25 features and 1 discrete response variable called `default_payment_next_month`. There are 30,000 observations of distinct credit card clients. Each row has a unique identifier called `ID`, representing a client. The other features include information amount a given client such as gender, age, approved credit limit, education, marital status, their past payment history, bill statements, and previous payments for 6 months. The history of past payment over 6 months (April-Sept 2005) is given by the features `pay_0`, `pay_2`, `pay_3`, ..., `pay_6` which take on numeric values representing the delay of the repayment in months, i.e. `pay_0`=1 means a client's payment is 1 month late in September 2005, `pay_6`=2 would then mean a clients payment was 2 months past payment in April 2005. A value of -1 is assigned for payments made on time in a given month. Note that the sequence is missing `pay_1`. We rename `pay_0` to `pay_1` for consistency and simplicity. We notice some of those features take on a value of -2, which is undocumented and therefore, we encode as 0.

Furthermore, the bill statements and previous payment monthly features (Sept-April 2005) are measured in dollar amounts. The mean value for the amount of a given credit card limit is $167,484. There is some imbalance in the target class, as well as education level and marital status. Further, the average age of clients is around 36. The target variable, `default_payment_next_month` takes on a value of 1 to indicate the client's payment is likely to default next month and 0 indicates non-defaults. For simplicity, we rename the target feature to `default`. Note that the months are reversed for the pay features. That is, `bill_amt1` and  `pay_amt1` will correspond to September 2005. However, in the past payment status features, the month of September is represented by `pay_6`, and April 2005 by `pay_0` which we rename to `pay_1` for consistency.
The `id` feature is dropped as it's irrelavant for our application. Further, unique identifiers that could be used to identify an individual would be dropped or decoded to protect the privacy of the individuals involvded.

The `education` feature takes on one of 7 numeric values representing a given client record education level. An education level with value 1 is assigned for clients with graduate degrees, 2 for bachelors degrees, 3 for high school, 4 for others (up to high school) with 5 and 6 as undefined. There is no definition for education level 0. We see that only 14 out of 30,000 observations correspond to clients with education level 0.

`marriage` takes on one of 3 values where a value of 1 indicates the client's marital status as married, 2 is single, and 3 is classified as any other status. `sex` takes on numeric values as well, with 1 indicating the client is male and 2 indicating the client is female.


# Data Cleaning

Before diving into EDA, we convert features into the best format for our application. We also explore any missing values and find none. Therefore, no imputation is required. However, we can consider applying a scaling transformation to the numeric features in the data set.

```{r clean data}
#clean column names
df <- dat %>% janitor::clean_names()
colnames(data) 

#convert feature types
factor_features <- c("id", "default_payment_next_month")
df[factor_features] <- lapply(df[factor_features], function(x) as.factor(x))

numeric_features <- c("limit_bal", "age", 
                      "pay_0", "pay_2", "pay_3", "pay_4", "pay_5", "pay_6",
                      "bill_amt1","bill_amt2", "bill_amt3", "bill_amt4", "bill_amt5", "bill_amt6",
                      "pay_amt1", "pay_amt2" , "pay_amt3" , "pay_amt4","pay_amt5", "pay_amt6")

df[numeric_features] <- lapply(df[numeric_features], function(x) as.numeric(x))

categorical_features <- c("id", "sex", "education", "marriage")
df[categorical_features] <- lapply(df[categorical_features], function(x) as.factor(x))

dmy <- dummyVars( ~ sex + education + marriage, data = df)
one_hot <- data.frame(predict(dmy, newdata = df))
one_hot <- lapply(one_hot, function(x) as.factor(x))
  
new_df <- cbind(df, one_hot)
new_df <- new_df %>% 
  janitor::clean_names() %>% 
  select(-c(sex, education, marriage))

#rename column and drop id feature
cred_data <- df %>% 
  rename(pay_1 = pay_0) %>%
  relocate(pay_1, .before=pay_2) %>%
  select(-id) 

#rename target column
cred_data <- cred_data %>%
  rename(default = default_payment_next_month)
  
#encode unknown value as 0
encode_function <- function(x){
  replace(x, x < -1, 0)
}
cred_data <- cred_data %>%
  mutate_at(c("pay_1", "pay_2", "pay_3", "pay_4", "pay_5", "pay_6"), encode_function)

#target class proportions
target <- cred_data$default
prop.table(table(target)) 
#training_data %>% group_by(default) %>% summarise(proportion = round(n()/nrow(.),4))

prop_plot <- cred_data %>% 
    ggplot(aes(x=as.numeric(default),  y = ..prop.., fill = factor(..x..), group = 1)) +
    geom_bar(stat='count') +
    geom_text(stat='count', aes(label=..prop..), vjust=3, hjust=0.5, color = 'black') +
    scale_y_continuous(labels = scales::percent_format()) + 
    ggtitle("Proportions of Defaulting Clients") +
    xlab("Defaults") +
    ylab("Proportion") 
  
prop_plot <- prop_plot + guides(fill=guide_legend(title="Default Flag")) 
prop_plot <- prop_plot + scale_shape_discrete(labels = c("Non-Defaults", "Defaults")) + scale_fill_discrete(labels = c("Non-Defaults", "Defaults"))

prop_plot
```

# Partition the data set into training and test sets

Before splitting the data set into training (75%) and testing (25%) sets, we inspect class balance to detect any imbalance in the target class which we attempt to correct. 

```{r split data}
# split into training and test data sets
training_rows <- cred_data %>% 
    select(default) %>% 
    pull() %>%
    createDataPartition(p = 0.75, list = FALSE)

training_data <- cred_data %>% slice(training_rows)
test_data <- cred_data %>% slice(-training_rows)

#testing statified split proportions
train_counts <- training_data$default
prop.table(table(train_counts)) 

test_counts <- test_data$default
prop.table(table(test_counts)) 
```


# Exploratory analysis on the training data set

## Correlation Analysis
```{r corr}
if (! require ("PerformanceAnalytics" )){
  install.packages ("PerformanceAnalytics")
  library (PerformanceAnalytics)
}
if (! require ("ggplot2" )){
  install.packages ("ggplot2")
  library (ggplot2)
}
if (! require ("GGally" )){
  install.packages ("GGally")
  library (GGally)
}
if (! require ("ggpubr" )){
  install.packages ("ggpubr")
  library (ggpubr)
}

numeric_df <- training_data
numeric_df$default <- as.numeric(numeric_df$default)
numeric_df$age <- as.numeric(numeric_df$age)
numeric_df$sex <- NULL
numeric_df$education <- NULL
numeric_df$marriage <- NULL

#ggcor plot
chart.Correlation(numeric_df, histogram=TRUE, method = "pearson", col="blue", pch=1, main="all")
``` 

Looking at the correlation plot, we see that the features `pay_1`, ..., `pay_6` are the most correlated with the target variable `default`. Demographic features in general do not seem to be highly correlated with our response but rather the features tracking the monthly bill amounts. The lowest correlated feature is `limit_bal`, which we applying a standardization transformation to before modeling.


## Feature Analysis

Next, we consider a feature selection method that allows for individual evaluation of each
feature. We apply the function selectKBest on the full dataset to select a subset for
modelling that utilizes the most significant features. To determine an optimal number of
features, or the best k, that will yield that strongest predictive powers, we first look at
the value per features attribute of all the features in the model. The top feature’s value,
`bill_amt1` is seen to be 82.1% and the least valuable feature. We consider selecting features with a value of approximately at least 70%.

```{r best predictors load}
if (! require ("FSinR" )){
  install.packages ("FSinR")
  library ( FSinR )
}
if (! require ("randomForest" )){
  install.packages ("randomForest")
  library ( randomForest )
}

if (! require ("VSURF")){
  install.packages ("VSURF")
  library (VSURF)
}
``` 


### Variable Importance 

The first approach we take to evaluate variable importance uses accuracy and gini importance.
```{r var imp}
#Variable Importance using Random Forests 
#method 1 - using library(RandomForest)
rf <- randomForest(default ~ ., data=training_data, ntree=50, mtry=2, importance=TRUE)
#50 is optimal number of trees
rf
varImpPlot(rf) 
pred1=predict(rf,newdata=test_data)
confusionMatrix(pred1, test_data$default) 

#method 2 - using library (VSURF)

#rf_vsruf <- VSURF(default_payment_next_month ~ ., data=training_data, ntree=50, mtry=2)  #long run-time
#print(rf_vsruf$varselect.pred)
#summary(rf_vsruf$varselect.pred)
#plot(rf_vsruf)
```
The importance of each variable in the random forest model is displayed in the figure
above. The importance function outputs a list of features, along with their corresponding
Mean Decrease Gini and Mean Decrease Accuracy values. Mean Decrease Accuracy suggests that if the variable is not important, then rearranging its values should not degrade the model’s prediction accuracy. The features at the top of the figure have the most predictive power in the model. Eliminating these features would significantly decrease the predictive power of the model. Once again here, we observe that demographic features has very little significance in the data set.

The second approach we take to evaluate variable importance uses the R package VSURF.
The recently publish library (2019) implements a 3-step feature selection process using random forests. For the sake of time, the team has decided to omit the work on VSURF variable importance until time permits.

Next we examine the distribution of the most and least important features. The plot suggests `education`, an interesting feature, is highly imbalanced. Most of the client's that make up this data set have education level 2 or 3 (university or high school), with about with education level 4.

It seems clients with lower credit limit balance are more likely to default the next payment.

```{r plots}
if (! require ("ggplot2" )){
  install.packages ("ggplot2")
  library (ggplot2)
}
if (! require ("GGally" )){
  install.packages ("GGally")
  library (GGally)
}
ggplot(training_data, aes(age, fill = default)) + 
  geom_histogram(binwidth = 6) + 
  facet_grid(.~education) + 
  theme_fivethirtyeight()

ggplot(training_data, aes(pay_1, fill = default)) + 
  geom_histogram(binwidth = 1) + 
  facet_grid(.~education) + 
  theme_fivethirtyeight()

ggplot(training_data) +
    aes(x = limit_bal,
        fill = default,
        color = default) +
    geom_density(alpha = 0.6) +
  xlab("Credit Limit") +
  ylab("Density") +
  ggtitle("Density of Credit Limit")
```

### Modeling Logistic Regression

```{r glm, warning=FALSE}
if (! require ("ROCR")){
  install.packages ("ROCR")
  library (ROCR)
}
library (ROCR)
glm_mode <- glm(default ~ ., data=training_data, family=binomial)
summary(glm_mode)

# drop target variable from testing set
test.x <-  test_data %>% 
  select(-default)

test_data$glm_score <- predict(glm_mode, type="response", test.x)
pred <- prediction(test_data$glm_score, test_data$default)
glm.perf <- performance(pred, "tpr", "fpr")

# box plot
test_data %>% 
  ggplot(aes(default, glm_score, fill=default)) +
  geom_boxplot() + 
  ggtitle("Logistic Regression Classifier - Scores distribution") +
  xlab("Default Payment") +
  ylab("Prediction")

# ROC
plot(glm.perf, lty=1, col="red", main="Logistic Regression ROC curve")

auc.perf <- performance(pred, measure = "auc") 

# Get accuracy and cutoff
acc.perf <- performance(pred, measure = "acc")
acc <- slot(acc.perf, "y.values")[[1]][which.max( slot(acc.perf, "y.values")[[1]] )]
```

The logistic regression classifier performed faily well on the test data with accuracy of 0.82 and AUC score of 0.724. However, many default payments were not detected. As such, we continue to investigate way to improve the models accuracy. 

